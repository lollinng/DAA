import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, k_means

df = pd.read_csv("sales_data_sample.csv",encoding='Latin') #Loading the dataset.
df.isnull().sum()

df_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'STATUS','POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'COUNTRY', 'ORDERDATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']
df = df.drop(df_drop, axis=1) 

df['PRODUCTLINE'].unique()
df['DEALSIZE'].unique()
productline = pd.get_dummies(df['PRODUCTLINE']) 
dealsize = pd.get_dummies(df['DEALSIZE'])
df = pd.concat([df,productline,dealsize], axis = 1)
df_drop  = ['PRODUCTLINE','DEALSIZE'] 
df = df.drop(df_drop, axis=1)
df['PRODUCTCODE'] = pd.Categorical(df['PRODUCTCODE']).codes

distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df)
    distortions.append(kmeanModel.inertia_) 
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

model = KMeans(n_clusters=3, random_state=2) 
model = model.fit(df) 
predictions = model.predict(df) 
predictions
unique, counts = np.unique(predictions,return_counts=True)
counts = counts.reshape(1,3)
counts_df = pd.DataFrame(counts,columns=['Cluster1','Cluster2','Cluster3'])
counts_df
labels = model.labels_
sales_of_cluster = pd.concat([df, pd.DataFrame({'cluster': labels})], axis=1)
sales_of_cluster.head()

u_labels = np.unique(labels)
for i in u_labels:
    plt.scatter(df['SALES'] , df['QUANTITYORDERED'],c=predictions)

plt.show()


# KNN
import pandas as pd
import numpy as np

df = pd.read_csv('diabetes.csv')
df.isnull().sum()
import matplotlib.pyplot as plt

X = df.drop('Outcome',axis = 1)
y1 = df['Outcome']

x = df[df['Outcome']==1]['BMI']
y = df[df['Outcome']==0]['BMI']
plt.hist([x, y], color=['red', 'green'], label = ['exit', 'not_exit'])

from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split
X = scale(X)
X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size = 0.3, random_state = 42)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

import sklearn.metrics as metrics
print("Confusion matrix: ")
tn, fp, fn, tp = metrics.confusion_matrix(y_test,y_pred).ravel()

accuracy  =(tp+tn)/(tp+tn+fp+fn)
error_rate = 1 - accuracy
precision =(tp)/(tp+fp)
recall  =(tp)/(tp+fn)
f1 =2*(( precision * recall)/( precision + recall))

print('Accuracy:\t',accuracy*100,
      '\nError rate:\t',error_rate*100,
    '\nPrecision:\t',precision*100,
    '\nRecall: \t',recall*100,
    '\nF1-Score:\t',f1*100)
    
print(metrics.classification_report(y_test,y_pred))